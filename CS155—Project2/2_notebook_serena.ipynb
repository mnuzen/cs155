{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 Sample Code\n",
    "\n",
    "This sample code is meant as a guide on how to use PyTorch and how to use the relevant model layers. This not a guide on how to design a network and the network in this example is intentionally designed to have poor performace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install torch==1.4.0+cpu torchvision==0.5.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
    "    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading MNIST\n",
    "The `torchvision` module contains links to many standard datasets. We can load the MNIST dataset into a `Dataset` object as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.5%5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.MNIST('./data', train=True, download=True,  # Downloads into a directory ../data\n",
    "                               transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=False,  # No need to download again\n",
    "                              transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Dataset` object is an iterable where each element is a tuple of (input `Tensor`, target):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 <class 'torch.Tensor'> <class 'int'>\n",
      "10000 <class 'torch.Tensor'> <class 'int'>\n",
      "2\n",
      "1\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset), type(train_dataset[0][0]), type(train_dataset[0][1]))\n",
    "print(len(test_dataset), type(test_dataset[0][0]), type(test_dataset[0][1]))\n",
    "a = len(train_dataset[0])\n",
    "b = len(train_dataset[0][0])\n",
    "c = len(train_dataset[0][0][0])\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert images to numpy arrays and plot them with matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x115a2bcf8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADgdJREFUeJzt3X9sXfV5x/HPs9D8QRoIXjUTpWFpIhQUIuZOJkwoGkXM5YeCggGhWkLKRBT3j1ii0hQNZX8MNAVFg2RqBKrsqqHJ1KWZBCghqpp0CZBOTBEmhF9mKQylqi2TFAWTH/zIHD/74x53Lvh+r3Pvufdc+3m/JMv3nuecex4d5ZPz8/pr7i4A8fxJ0Q0AKAbhB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8Q1GWNXJmZ8TghUGfublOZr6Y9v5ndYWbHzex9M3ukls8C0FhW7bP9ZjZL0m8kdUgalPSqpC53H0gsw54fqLNG7PlXSHrf3T9w9wuSfi5pdQ2fB6CBagn/Akm/m/B+MJv2R8ys28z6zay/hnUByFndL/i5e5+kPonDfqCZ1LLnH5K0cML7b2bTAEwDtYT/VUnXmtm3zGy2pO9J2ptPWwDqrerDfncfNbMeSfslzZK03d3fya0zAHVV9a2+qlbGOT9Qdw15yAfA9EX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUFUP0S1JZnZC0llJFyWNunt7Hk0hP7NmzUrWr7zyyrquv6enp2zt8ssvTy67dOnSZH39+vXJ+pNPPlm21tXVlVz2888/T9Y3b96crD/22GPJejOoKfyZW939oxw+B0ADcdgPBFVr+F3SATN7zcy682gIQGPUeti/0t2HzOzPJP3KzP7b3Q9PnCH7T4H/GIAmU9Oe392Hst+nJD0vacUk8/S5ezsXA4HmUnX4zWyOmc0dfy3pu5LezqsxAPVVy2F/q6TnzWz8c/7N3X+ZS1cA6q7q8Lv7B5L+IsdeZqxrrrkmWZ89e3ayfvPNNyfrK1euLFubN29ectn77rsvWS/S4OBgsr5t27ZkvbOzs2zt7NmzyWXfeOONZP3ll19O1qcDbvUBQRF+ICjCDwRF+IGgCD8QFOEHgjJ3b9zKzBq3sgZqa2tL1g8dOpSs1/trtc1qbGwsWX/ooYeS9XPnzlW97uHh4WT9448/TtaPHz9e9brrzd1tKvOx5weCIvxAUIQfCIrwA0ERfiAowg8ERfiBoLjPn4OWlpZk/ciRI8n64sWL82wnV5V6HxkZSdZvvfXWsrULFy4kl436/EOtuM8PIInwA0ERfiAowg8ERfiBoAg/EBThB4LKY5Te8E6fPp2sb9iwIVlftWpVsv76668n65X+hHXKsWPHkvWOjo5k/fz588n69ddfX7b28MMPJ5dFfbHnB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgKn6f38y2S1ol6ZS7L8+mtUjaLWmRpBOSHnD39B8618z9Pn+trrjiimS90nDSvb29ZWtr165NLvvggw8m67t27UrW0Xzy/D7/TyXd8aVpj0g66O7XSjqYvQcwjVQMv7sflvTlR9hWS9qRvd4h6Z6c+wJQZ9We87e6+/h4Rx9Kas2pHwANUvOz/e7uqXN5M+uW1F3regDkq9o9/0kzmy9J2e9T5WZ09z53b3f39irXBaAOqg3/XklrstdrJO3Jpx0AjVIx/Ga2S9J/SVpqZoNmtlbSZkkdZvaepL/J3gOYRiqe87t7V5nSbTn3EtaZM2dqWv6TTz6petl169Yl67t3707Wx8bGql43isUTfkBQhB8IivADQRF+ICjCDwRF+IGgGKJ7BpgzZ07Z2gsvvJBc9pZbbknW77zzzmT9wIEDyToajyG6ASQRfiAowg8ERfiBoAg/EBThB4Ii/EBQ3Oef4ZYsWZKsHz16NFkfGRlJ1l988cVkvb+/v2zt6aefTi7byH+bMwn3+QEkEX4gKMIPBEX4gaAIPxAU4QeCIvxAUNznD66zszNZf+aZZ5L1uXPnVr3ujRs3Jus7d+5M1oeHh5P1qLjPDyCJ8ANBEX4gKMIPBEX4gaAIPxAU4QeCqnif38y2S1ol6ZS7L8+mPSppnaTfZ7NtdPdfVFwZ9/mnneXLlyfrW7duTdZvu636kdx7e3uT9U2bNiXrQ0NDVa97OsvzPv9PJd0xyfR/cfe27Kdi8AE0l4rhd/fDkk43oBcADVTLOX+Pmb1pZtvN7KrcOgLQENWG/0eSlkhqkzQsaUu5Gc2s28z6zaz8H3MD0HBVhd/dT7r7RXcfk/RjSSsS8/a5e7u7t1fbJID8VRV+M5s/4W2npLfzaQdAo1xWaQYz2yXpO5K+YWaDkv5R0nfMrE2SSzoh6ft17BFAHfB9ftRk3rx5yfrdd99dtlbpbwWYpW9XHzp0KFnv6OhI1mcqvs8PIInwA0ERfiAowg8ERfiBoAg/EBS3+lCYL774Ilm/7LL0Yyijo6PJ+u2331629tJLLyWXnc641QcgifADQRF+ICjCDwRF+IGgCD8QFOEHgqr4fX7EdsMNNyTr999/f7J+4403lq1Vuo9fycDAQLJ++PDhmj5/pmPPDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBcZ9/hlu6dGmy3tPTk6zfe++9yfrVV199yT1N1cWLF5P14eHhZH1sbCzPdmYc9vxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EFTF+/xmtlDSTkmtklxSn7v/0MxaJO2WtEjSCUkPuPvH9Ws1rkr30ru6usrWKt3HX7RoUTUt5aK/vz9Z37RpU7K+d+/ePNsJZyp7/lFJf+fuyyT9laT1ZrZM0iOSDrr7tZIOZu8BTBMVw+/uw+5+NHt9VtK7khZIWi1pRzbbDkn31KtJAPm7pHN+M1sk6duSjkhqdffx5ys/VOm0AMA0MeVn+83s65KelfQDdz9j9v/Dgbm7lxuHz8y6JXXX2iiAfE1pz29mX1Mp+D9z9+eyySfNbH5Wny/p1GTLunufu7e7e3seDQPIR8XwW2kX/xNJ77r71gmlvZLWZK/XSNqTf3sA6qXiEN1mtlLSryW9JWn8O5IbVTrv/3dJ10j6rUq3+k5X+KyQQ3S3tqYvhyxbtixZf+qpp5L166677pJ7ysuRI0eS9SeeeKJsbc+e9P6Cr+RWZ6pDdFc853f3/5RU7sNuu5SmADQPnvADgiL8QFCEHwiK8ANBEX4gKMIPBMWf7p6ilpaWsrXe3t7ksm1tbcn64sWLq+opD6+88kqyvmXLlmR9//79yfpnn312yT2hMdjzA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQYe7z33TTTcn6hg0bkvUVK1aUrS1YsKCqnvLy6aeflq1t27Ytuezjjz+erJ8/f76qntD82PMDQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFBh7vN3dnbWVK/FwMBAsr5v375kfXR0NFlPfed+ZGQkuSziYs8PBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0GZu6dnMFsoaaekVkkuqc/df2hmj0paJ+n32awb3f0XFT4rvTIANXN3m8p8Uwn/fEnz3f2omc2V9JqkeyQ9IOmcuz851aYIP1B/Uw1/xSf83H1Y0nD2+qyZvSup2D9dA6Bml3TOb2aLJH1b0pFsUo+ZvWlm283sqjLLdJtZv5n119QpgFxVPOz/w4xmX5f0sqRN7v6cmbVK+kil6wD/pNKpwUMVPoPDfqDOcjvnlyQz+5qkfZL2u/vWSeqLJO1z9+UVPofwA3U21fBXPOw3M5P0E0nvTgx+diFwXKekty+1SQDFmcrV/pWSfi3pLUlj2eSNkroktal02H9C0vezi4Opz2LPD9RZrof9eSH8QP3ldtgPYGYi/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBNXoIbo/kvTbCe+/kU1rRs3aW7P2JdFbtfLs7c+nOmNDv8//lZWb9bt7e2ENJDRrb83al0Rv1SqqNw77gaAIPxBU0eHvK3j9Kc3aW7P2JdFbtQrprdBzfgDFKXrPD6AghYTfzO4ws+Nm9r6ZPVJED+WY2Qkze8vMjhU9xFg2DNopM3t7wrQWM/uVmb2X/Z50mLSCenvUzIaybXfMzO4qqLeFZvaimQ2Y2Ttm9nA2vdBtl+irkO3W8MN+M5sl6TeSOiQNSnpVUpe7DzS0kTLM7ISkdncv/J6wmf21pHOSdo6PhmRm/yzptLtvzv7jvMrd/75JentUlzhyc516Kzey9N+qwG2X54jXeShiz79C0vvu/oG7X5D0c0mrC+ij6bn7YUmnvzR5taQd2esdKv3jabgyvTUFdx9296PZ67OSxkeWLnTbJfoqRBHhXyDpdxPeD6q5hvx2SQfM7DUz6y66mUm0ThgZ6UNJrUU2M4mKIzc30pdGlm6abVfNiNd544LfV61097+UdKek9dnhbVPy0jlbM92u+ZGkJSoN4zYsaUuRzWQjSz8r6QfufmZirchtN0lfhWy3IsI/JGnhhPffzKY1BXcfyn6fkvS8SqcpzeTk+CCp2e9TBffzB+5+0t0vuvuYpB+rwG2XjSz9rKSfuftz2eTCt91kfRW13YoI/6uSrjWzb5nZbEnfk7S3gD6+wszmZBdiZGZzJH1XzTf68F5Ja7LXayTtKbCXP9IsIzeXG1laBW+7phvx2t0b/iPpLpWu+P+PpH8ooocyfS2W9Eb2807RvUnapdJh4P+qdG1kraQ/lXRQ0nuS/kNSSxP19q8qjeb8pkpBm19QbytVOqR/U9Kx7Oeuorddoq9CthtP+AFBccEPCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQ/weCC5r/92q6mAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_dataset[0][0][0].numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Definition\n",
    "Let's instantiate a model and take a look at the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Flatten()\n",
      "  (1): Linear(in_features=784, out_features=20, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Dropout(p=0.5, inplace=False)\n",
      "  (4): Linear(in_features=20, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    # In problem 2, we don't use the 2D structure of an image at all. Our network\n",
    "    # takes in a flat vector of the pixel values as input.\n",
    "    nn.Flatten(),  \n",
    "    nn.Linear(784, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "We also choose an optimizer and a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could write our training procedure manually and directly index the `Dataset` objects, but the `DataLoader` object conveniently creates an iterable for automatically creating random minibatches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now write our backpropagation loop, training for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1  Loss: 0.4499\n",
      "Train Epoch: 2  Loss: 0.6528\n",
      "Train Epoch: 3  Loss: 0.5716\n",
      "Train Epoch: 4  Loss: 0.5873\n",
      "Train Epoch: 5  Loss: 0.5245\n",
      "Train Epoch: 6  Loss: 0.4954\n",
      "Train Epoch: 7  Loss: 0.5398\n",
      "Train Epoch: 8  Loss: 0.7258\n",
      "Train Epoch: 9  Loss: 0.5781\n",
      "Train Epoch: 10  Loss: 0.3674\n"
     ]
    }
   ],
   "source": [
    "# Some layers, such as Dropout, behave differently during training\n",
    "model.train()\n",
    "\n",
    "for epoch in range(10):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Erase accumulated gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(output, target)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Weight update\n",
    "        optimizer.step()\n",
    "\n",
    "    # Track loss each epoch\n",
    "    print('Train Epoch: %d  Loss: %.4f' % (epoch + 1,  loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "We can perform forward passes through the network without saving gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0087, Accuracy: 9248/10000 (92.4800)\n"
     ]
    }
   ],
   "source": [
    "# Putting layers like Dropout into evaluation mode\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "# Turning off automatic differentiation\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        test_loss += loss_fn(output, target).item()  # Sum up batch loss\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max class score\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print('Test set: Average loss: %.4f, Accuracy: %d/%d (%.4f)' %\n",
    "      (test_loss, correct, len(test_loader.dataset),\n",
    "       100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, batch_sz, num_epochs):\n",
    "    # choose optimizer to be Adama and loss function to be Cross Entropy\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_sz, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_sz, shuffle=True) \n",
    "\n",
    "    # Some layers, such as Dropout, behave differently during training\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # Erase accumulated gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(output, target)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Weight update\n",
    "            optimizer.step()\n",
    "\n",
    "        # Track loss each epoch\n",
    "        print('Train Epoch: %d  Loss: %.4f' % (epoch + 1,  loss.item()))\n",
    "\n",
    "    # Putting layers like Dropout into evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    # Turning off automatic differentiation\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target).item()  # Sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max class score\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('Test set: Average loss: %.4f, Accuracy: %d/%d (%.4f)' %\n",
    "          (test_loss, correct, len(test_loader.dataset),\n",
    "           100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1  Loss: 0.1016\n",
      "Train Epoch: 2  Loss: 0.2973\n",
      "Train Epoch: 3  Loss: 0.0365\n",
      "Train Epoch: 4  Loss: 0.0383\n",
      "Train Epoch: 5  Loss: 0.0363\n",
      "Train Epoch: 6  Loss: 0.0180\n",
      "Train Epoch: 7  Loss: 0.0165\n",
      "Train Epoch: 8  Loss: 0.0367\n",
      "Train Epoch: 9  Loss: 0.0220\n",
      "Train Epoch: 10  Loss: 0.0022\n",
      "Test set: Average loss: 0.0026, Accuracy: 9762/10000 (97.6200)\n"
     ]
    }
   ],
   "source": [
    "model2c = nn.Sequential(\n",
    "    # In problem 2, we don't use the 2D structure of an image at all. Our network\n",
    "    # takes in a flat vector of the pixel values as input.\n",
    "    nn.Flatten(),  \n",
    "    nn.Linear(784, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 10)\n",
    ")\n",
    "train_model(model2c, 32, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1  Loss: 0.1886\n",
      "Train Epoch: 2  Loss: 0.3007\n",
      "Train Epoch: 3  Loss: 0.1198\n",
      "Train Epoch: 4  Loss: 0.0228\n",
      "Train Epoch: 5  Loss: 0.0559\n",
      "Train Epoch: 6  Loss: 0.4365\n",
      "Train Epoch: 7  Loss: 0.2096\n",
      "Train Epoch: 8  Loss: 0.0863\n",
      "Train Epoch: 9  Loss: 0.1786\n",
      "Train Epoch: 10  Loss: 0.0609\n",
      "Test set: Average loss: 0.0022, Accuracy: 9812/10000 (98.1200)\n"
     ]
    }
   ],
   "source": [
    "model2d = nn.Sequential(\n",
    "    # In problem 2, we don't use the 2D structure of an image at all. Our network\n",
    "    # takes in a flat vector of the pixel values as input.\n",
    "    nn.Flatten(),  \n",
    "    nn.Linear(784, 120),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(120, 80),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(80, 10)\n",
    "    \n",
    ")\n",
    "train_model(model2d, 32, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1  Loss: 0.0036\n",
      "Train Epoch: 2  Loss: 0.0005\n",
      "Train Epoch: 3  Loss: 0.0100\n",
      "Train Epoch: 4  Loss: 0.0493\n",
      "Train Epoch: 5  Loss: 0.0311\n",
      "Train Epoch: 6  Loss: 0.0261\n",
      "Train Epoch: 7  Loss: 0.0019\n",
      "Train Epoch: 8  Loss: 0.0039\n",
      "Train Epoch: 9  Loss: 0.0065\n",
      "Train Epoch: 10  Loss: 0.0064\n",
      "Test set: Average loss: 0.0006, Accuracy: 9832/10000 (98.3200)\n"
     ]
    }
   ],
   "source": [
    "model2e = nn.Sequential(\n",
    "    # In problem 2, we don't use the 2D structure of an image at all. Our network\n",
    "    # takes in a flat vector of the pixel values as input.\n",
    "    nn.Flatten(),  \n",
    "    nn.Linear(784, 700),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(700, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(200, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(100, 10)\n",
    ")\n",
    "train_model(model2d, 200, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
